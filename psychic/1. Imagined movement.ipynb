{
 "metadata": {
  "name": "",
  "signature": "sha256:20cafbd0883bf21a5cd92921fdaf164c92b64cad034ca869b3d6c38c41965f62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Imagined movement with Psychic\n",
      "\n",
      "In this tutorial we will look at imagined movement. Our movement is controlled in the motor cortex where there is an increased level of mu activity (8\u201312 Hz) when we perform movements. This is accompanied by a reduction of this mu activity in specific regions that deal with the limb that is currently moving. This decrease is called Event Related Desynchronization (ERD). By measuring the amount of mu activity at different locations on the motor cortex, we can determine which limb the subject is moving. This effect also occurs when the subject is not actually moving his limbs, but merely imagining it.\n",
      "\n",
      "## Obtaining the data\n",
      "The dataset for this tutorial is provided by the fourth BCI competition,\n",
      "which you will have to download youself. First, go to http://www.bbci.de/competition/iv/#download\n",
      "and fill in your name and email address. An email will be sent to you\n",
      "automatically containing a username and password for the download area.\n",
      "\n",
      "Download Data Set 1, from Berlin, the 100Hz version in MATLAB format:\n",
      "http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip\n",
      "and unzip it in a subdirectory called 'data_set_IV'. This subdirectory\n",
      "should be inside the directory in which you've stored the tutorial files.\n",
      "\n",
      "Also download the true labels of this dataset:\n",
      "http://bbci.de/competition/iii/results/berlin_IVa/true_labels_al.mat \n",
      "and store them in the same 'data_set_IV' subdirectory you created\n",
      "earlier.\n",
      " \n",
      "[Description of the data](http://bbci.de/competition/iv/desc_1.html)\n",
      "\n",
      "If you've followed the instructions above, the following code should load\n",
      "the data:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we load the BCI competition data, assuming it still exists at the location it was downloaded in the previous tutorial. It is stored in a Psychic dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%reset\n",
      "import psychic\n",
      "import numpy as np\n",
      "import scipy.io\n",
      "\n",
      "m1 = scipy.io.loadmat('data_set_IV/BCICIV_calib_ds1a.mat', struct_as_record=True)\n",
      "\n",
      "data = np.c_[m1['cnt'].T, m2['cnt'].T]\n",
      "nchannels, nsamples = data.shape\n",
      "\n",
      "event_onsets = m1['mrk'][0][0][0]\n",
      "event_codes = m1['mrk'][0][0][1]\n",
      "labels = np.zeros((1, nsamples), int)\n",
      "labels[0, event_onsets] = event_codes\n",
      "\n",
      "sample_rate = m['nfo']['fs'][0][0][0][0]\n",
      "ids = np.arange(nsamples) / float(sample_rate)\n",
      "\n",
      "feat_lab = [s[0].encode('utf8') for s in m['nfo']['clab'][0][0][0]]\n",
      "\n",
      "d = psychic.DataSet(data, labels, ids, feat_lab=feat_lab)\n",
      "print d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 431152 instances, 59 features [59], 3 classes: [100, 430952, 100], extras: []\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This dataset represents 431152 samples of ongoing EEG data. Each sample is called an instance and has 59 features: one for each channel. Like in the previous tutorial, the data is sliced into trials by defining a marker dictionary containing class labels for each event code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trials = psychic.nodes.Slice({-1: 'right', 1:'foot'}, (0.5, 2.5)).train_apply(d)\n",
      "print trials"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 200 instances, 11800 features [59x200], 2 classes: [100, 100], extras: []\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each instance is now a trial, so `trials.data` is 3-dimensional: channels x samples x trials. `trials.Y` contains the class labels: there are 140 trials of each class. Again, check back with tutorial 2 if this founds unfamiliar.\n",
      "\n",
      "Much of the functionality in Psychic comes in the form of 'nodes'. A node is a class with two methods: `train` and `apply`. The `train` method will configure the node based on some sample data. The `apply` method will apply the transformation the node was designed to do on some data.\n",
      "\n",
      "The first step is to use a bandpass filter on the data, filtering the signal in the 8-15Hz range. This is done by constructing a `psychic.nodes.Filter` node, that takes a filter design function as parameter. We use the same filter design function as in the previous tutorial. Next we call the `train` method of the node that uses the filter design function to design the filter. Finally, we can use `apply` to actually filter the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct a filter node, using a filter design function\n",
      "filt = psychic.nodes.Filter(lambda s: scipy.signal.iirfilter(6, [8/(s/2.0), 15/(s/2.0)]))\n",
      "\n",
      "# Use the filter design on the data, the result is stored internally\n",
      "filt.train(trials)\n",
      "\n",
      "# Now the filter is ready for use\n",
      "trials_filt = filt.apply(trials)\n",
      "print trials_filt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 200 instances, 11800 features [59x200], 2 classes: [100, 100], extras: []\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "before moving on, we need to split the data into a train and a test set. Remember that we calculate the CSP and train the classifier on the training data and check the performance on the test data.\n",
      "\n",
      "Psychic offers the `psychic.cv.strat_splits` function that makes splitting data easy. It will create so called `splits` containing randomly chosen trials, while making sure that each split contains an equal amount of trials for each class. Below, the data is split into two splits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train, test = golem.cv.strat_splits(trials_filt, 2)\n",
      "\n",
      "print train\n",
      "print test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 100 instances, 11800 features [59x200], 2 classes: [50, 50], extras: []\n",
        "DataSet with 100 instances, 11800 features [59x200], 2 classes: [50, 50], extras: []\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that both the train and the test set have an equal number of trials belonging to each class (50).\n",
      "\n",
      "Recall from the previous tutorial the steps we performed to classify the trials:\n",
      "\n",
      " 1. Calculate the CSP and drop everything but the first and last CSP component\n",
      " 2. Calculate the logvar of those components\n",
      " 3. Classify the trial using Linear Discriminant Analysis\n",
      "\n",
      "Nodes can be linked together forming a chain. Conveniently, scikit-learn nodes can be included as well. The code below constructs three nodes that implement the steps above and links them up as a chain:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.lda import LDA\n",
      "\n",
      "pipeline = psychic.nodes.Chain([\n",
      "    # Only use the 2 most useful CSP components (first and last)\n",
      "    psychic.nodes.spatialfilter.CSP(2), \n",
      "    \n",
      "    # ApplyOverInstances applies the given function to the instances. We give it a function that calculates\n",
      "    # the logvar of each component\n",
      "    psychic.nodes.ApplyOverInstances(lambda x: np.log(np.var(x, axis=1))), # \n",
      "    \n",
      "    # Finally the Linear Discriminant Analysis\n",
      "    LDA(),\n",
      "])\n",
      "\n",
      "# Train the pipeline using the training data\n",
      "pipeline.train(train)\n",
      "\n",
      "# Apply the pipeline on the test data\n",
      "result = pipeline.apply(test)\n",
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DataSet with 100 instances, 2 features [2], 2 classes: [50, 50], extras: []\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`result.data` contains the output of the LDA: the predicted class labels. `result.labels` still contains the true class labels. The code below uses this to calculate the accuracy and the confusion matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print psychic.perf.accuracy(result)\n",
      "psychic.perf.conf_mat(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.83\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "array([[ 37.,  13.],\n",
        "       [  4.,  46.]])"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result may vary every time you run the code, since the train and test set are random splits of the data. It should however be around 80%."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}